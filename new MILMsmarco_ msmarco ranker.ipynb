{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8f9994-bf24-4eed-abaf-9e74fb901322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[out] OUT_DIR = /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1\n",
      "[build] Building matrices from scratch | device=cuda | K=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb8174d92a040319d5a6169bf808ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53045786cadb440f9b1f4adf83ca499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584544d081c9449083cf542d2e1f0551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f477309c7d4944029aa5ed46c41ec3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c135c5ae94a34e41b41091c75fdebff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20951011eb145b9a5aa0c41f5375dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build] Reranker hidden_size D=384\n",
      "[build] queries=800 (we build 2 runs per query => 2*Q runs)\n",
      "[build] 50/800 queries done\n",
      "[build] 100/800 queries done\n",
      "[build] 150/800 queries done\n",
      "[build] 200/800 queries done\n",
      "[build] 250/800 queries done\n",
      "[build] 300/800 queries done\n",
      "[build] 350/800 queries done\n",
      "[build] 400/800 queries done\n",
      "[build] 450/800 queries done\n",
      "[build] 500/800 queries done\n",
      "[build] 550/800 queries done\n",
      "[build] 600/800 queries done\n",
      "[build] 650/800 queries done\n",
      "[build] 700/800 queries done\n",
      "[build] 750/800 queries done\n",
      "[build] 800/800 queries done\n",
      "[build] saved /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/dataset_with_chunkids_cross-encoder_ms-marco-MiniLM-L6-v2_K5.npz\n",
      "        X=(1600, 5, 384) y=(1600,) unique_qids=800\n",
      "[data] X=(1600, 5, 384) y=(1600,) unique_qids=800\n",
      "[split] train=1120 val=240 test=240\n",
      "[ep 01] train_loss=1.605589 val_runBCE=0.123899\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 02] train_loss=1.411889 val_runBCE=0.122912\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 03] train_loss=1.412369 val_runBCE=0.113967\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 04] train_loss=1.438825 val_runBCE=0.124462\n",
      "[ep 05] train_loss=1.369777 val_runBCE=0.136960\n",
      "[ep 06] train_loss=1.387824 val_runBCE=0.087575\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 07] train_loss=1.353435 val_runBCE=0.085696\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 08] train_loss=1.343892 val_runBCE=0.097797\n",
      "[ep 09] train_loss=1.345418 val_runBCE=0.087098\n",
      "[ep 10] train_loss=1.341825 val_runBCE=0.201373\n",
      "[ep 11] train_loss=1.418057 val_runBCE=0.073455\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 12] train_loss=1.349014 val_runBCE=0.080746\n",
      "[ep 13] train_loss=1.323879 val_runBCE=0.073315\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 14] train_loss=1.304712 val_runBCE=0.049521\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 15] train_loss=1.290916 val_runBCE=0.081670\n",
      "[ep 16] train_loss=1.292440 val_runBCE=0.056373\n",
      "[ep 17] train_loss=1.319590 val_runBCE=0.037733\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 18] train_loss=1.296707 val_runBCE=0.064291\n",
      "[ep 19] train_loss=1.268009 val_runBCE=0.040845\n",
      "[ep 20] train_loss=1.283488 val_runBCE=0.040711\n",
      "[ep 21] train_loss=1.276029 val_runBCE=0.039374\n",
      "[ep 22] train_loss=1.270501 val_runBCE=0.074409\n",
      "[ep 23] train_loss=1.276677 val_runBCE=0.033062\n",
      "  -> saved BEST: /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/BEST_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5.pth\n",
      "[ep 24] train_loss=1.266706 val_runBCE=0.061572\n",
      "[ep 25] train_loss=1.255149 val_runBCE=0.046197\n",
      "[ep 26] train_loss=1.267432 val_runBCE=0.050316\n",
      "[ep 27] train_loss=1.260495 val_runBCE=0.040150\n",
      "[ep 28] train_loss=1.278053 val_runBCE=0.066464\n",
      "[ep 29] train_loss=1.270359 val_runBCE=0.040686\n",
      "[ep 30] train_loss=1.261537 val_runBCE=0.045614\n",
      "[ep 31] train_loss=1.266336 val_runBCE=0.040756\n",
      "[ep 32] train_loss=1.271749 val_runBCE=0.033708\n",
      "[ep 33] train_loss=1.270701 val_runBCE=0.066032\n",
      "[ep 34] train_loss=1.333227 val_runBCE=0.037145\n",
      "[ep 35] train_loss=1.241945 val_runBCE=0.063315\n",
      "[calib] saved /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/calibration_cross-encoder_ms-marco-MiniLM-L6-v2.json\n",
      "\n",
      "=== TEST REPORT (run + chunk eval-only, GATED ONLY) ===\n",
      "{'run_level': {'counts': {'tp': 120, 'tn': 119, 'fp': 1, 'fn': 0}, 'rates': {'acc': 0.9958333333333333, 'tpr': 0.9999999999916667, 'fpr': 0.008333333333263889, 'prec': 0.9917355371818865}, 'thr_det': 0.5, 'pooling': 'AttentionMIL(Ilse2018)'}, 'chunk_level_eval_only_gated': {'counts': {'tp': 551, 'tn': 595, 'fp': 54, 'fn': 0}, 'rates': {'acc': 0.955, 'tpr': 0.9999999999981851, 'fpr': 0.08320493066242958, 'prec': 0.9107438016513872}, 'localization': 'threshold(thr_loc=0.020000)', 'gating': 'Only localize when run_pred==1', 'gt_rule': \"eval-only: ('gpt' in chunk_id) for malicious runs; all-0 for benign runs\"}}\n",
      "[report] saved /workspace/newrags/poisoned_msmarco/msmarco_MIL_SetTransformer_GATEDONLY_cross-encoder_ms-marco-MiniLM-L6-v2_K5_v1/test_report_gated_only_cross-encoder_ms-marco-MiniLM-L6-v2.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ONE-MODEL, GATED-ONLY DEFENSE PIPELINE (build matrices from scratch)\n",
    "#\n",
    "# - Builds CLS matrices M_q ∈ R^{K×D} from scratch:\n",
    "#     Contriever retrieve -> cross-encoder rerank -> take last-layer [CLS]\n",
    "# - Trains a SINGLE MIL model with RUN labels ONLY (no chunk labels in train/val)\n",
    "#     SetTransformer encoder + Attention-MIL pooling\n",
    "# - Defense is GATED:\n",
    "#     If run_pred==0 => do NOT localize/remove any chunks\n",
    "#     If run_pred==1 => localize chunks using scores s_i\n",
    "# - Chunk labels are used ONLY at TEST time for eval-only reporting:\n",
    "#     gt_rule: (\"gpt\" in chunk_id) for malicious runs; all-0 for benign runs\n",
    "#\n",
    "# Outputs saved to a NEW folder.\n",
    "# ============================================================\n",
    "\n",
    "import os, json, random, math\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "SEED = 7\n",
    "\n",
    "K = 5  # top-k reranked passages per run  (CHANGED: was 10)\n",
    "RETRIEVER_MODEL_NAME = \"facebook/contriever\"\n",
    "\n",
    "# ✅ CHANGED: BGE -> MiniLM cross-encoder reranker\n",
    "RERANKER_MODEL_NAME  = \"cross-encoder/ms-marco-MiniLM-L6-v2\"\n",
    "\n",
    "# ✅ NEW: tag for output folder/filenames (sanitized to be filesystem-safe)\n",
    "RERANKER_TAG = \"cross-encoder_ms-marco-MiniLM-L6-v2\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Paths (EDIT if needed) ----\n",
    "CHUNKS_JSON_PATH_BENIGN = \"/workspace/newrags/msmarcorag/msmarco_contriever_chunks.json\"\n",
    "EMB_JSON_PATH_BENIGN    = \"/workspace/newrags/msmarcorag/msmarco_contriever_embeddings_contriever.json\"\n",
    "\n",
    "BASE_DIR_MAL = \"/workspace/newrags/poisoned_msmarco\"\n",
    "CHUNKS_JSON_PATH_MAL = f\"{BASE_DIR_MAL}/msmarco_contriever_chunks_poisoned.json\"\n",
    "EMB_JSON_PATH_MAL    = f\"{BASE_DIR_MAL}/msmarco_contriever_embeddings_poisoned2.json\"\n",
    "\n",
    "QUERY_LIST_PATH = f\"{BASE_DIR_MAL}/successful_poisoned_msmarco.json\"\n",
    "\n",
    "# ---- NEW output folder (different from previous runs) ----\n",
    "# ✅ CHANGED: include reranker name/tag in the output directory\n",
    "OUT_DIR = Path(f\"{BASE_DIR_MAL}/msmarco_MIL_SetTransformer_GATEDONLY_{RERANKER_TAG}_K{K}_v1\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ✅ CHANGED: include reranker tag in filenames (keeps artifacts distinct)\n",
    "DATASET_NPZ   = OUT_DIR / f\"dataset_with_chunkids_{RERANKER_TAG}_K{K}.npz\"\n",
    "BEST_CKPT_PTH = OUT_DIR / f\"BEST_MIL_SetTransformer_GATEDONLY_{RERANKER_TAG}_K{K}.pth\"\n",
    "REPORT_JSON   = OUT_DIR / f\"test_report_gated_only_{RERANKER_TAG}.json\"\n",
    "CALIB_JSON    = OUT_DIR / f\"calibration_{RERANKER_TAG}.json\"\n",
    "\n",
    "# ---- Build matrices from scratch ----\n",
    "BUILD_MATRICES = True\n",
    "FORCE_REBUILD  = True  # set False if you want to reuse DATASET_NPZ\n",
    "\n",
    "# ---- Split ----\n",
    "TRAIN_SPLIT = 0.70\n",
    "VAL_SPLIT   = 0.15  # remaining is test\n",
    "\n",
    "# ---- Training ----\n",
    "BATCH_SIZE   = 32\n",
    "EPOCHS       = 35\n",
    "LR           = 2e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP    = 1.0\n",
    "\n",
    "# ---- Model ----\n",
    "D_MODEL  = 256\n",
    "N_HEADS  = 8\n",
    "N_LAYERS = 3\n",
    "DROPOUT  = 0.15\n",
    "EMBED_DIM = 128  # for SupCon\n",
    "\n",
    "# ---- Loss knobs (RUN labels only) ----\n",
    "W_DET = 1.0\n",
    "\n",
    "# Positive coverage (helps chunk TPR but can raise FPR if too strong)\n",
    "W_COV   = 0.60\n",
    "RHO_COV = 0.55\n",
    "\n",
    "# Strong benign suppression (main lever for reducing chunk FPR)\n",
    "W_NEG_MEAN = 1.20\n",
    "W_NEG_MAX  = 0.80\n",
    "W_NEG_LSE  = 0.50  # tail penalty on benign\n",
    "\n",
    "# Prevent attention collapse on positives (useful when many chunks poisoned)\n",
    "W_ATTN_ENT = 0.20\n",
    "\n",
    "# Optional SupCon (still only run labels)\n",
    "USE_SUPCON = True\n",
    "W_SUPCON   = 0.50\n",
    "SUPCON_TAU = 0.2\n",
    "\n",
    "# ---- Thresholds ----\n",
    "THR_DET = 0.5\n",
    "\n",
    "# Calibrate thr_loc using benign VAL only (no chunk labels), then clamp to avoid crazy tiny thresholds\n",
    "TARGET_CHUNK_FPR_BENIGN_VAL = 0.10  # desired fraction of benign chunks flagged (before gating)\n",
    "MIN_THR_LOC = 0.02                 # clamp low\n",
    "MAX_THR_LOC = 0.98                 # clamp high\n",
    "\n",
    "# Alternative: remove top-r chunks for flagged runs (more stable than thresholds).\n",
    "USE_TOPR_INSTEAD_OF_THRESHOLD = False\n",
    "TOP_R = 2  # remove top 2 chunks when run_pred==1 (only used if USE_TOPR...=True)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# JSON / data helpers\n",
    "# ============================================================\n",
    "def _load_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _parse_queries(obj) -> list:\n",
    "    if not isinstance(obj, list):\n",
    "        raise ValueError(\"QUERY_LIST_PATH must be a JSON list.\")\n",
    "    out = []\n",
    "    for it in obj:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        qid = it.get(\"qid\", it.get(\"query_id\", it.get(\"id\", None)))\n",
    "        q   = it.get(\"query\", it.get(\"question\", it.get(\"text\", None)))\n",
    "        if qid is None or q is None:\n",
    "            continue\n",
    "        out.append({\"qid\": int(qid), \"query\": str(q)})\n",
    "    if not out:\n",
    "        raise ValueError(\"Could not parse any (qid, query) items from QUERY_LIST_PATH.\")\n",
    "    return out\n",
    "\n",
    "def _get_chunk_id(chunk: dict) -> str:\n",
    "    for k in [\"chunk_id\", \"id\", \"_id\", \"docid\", \"passage_id\"]:\n",
    "        if k in chunk:\n",
    "            return str(chunk[k])\n",
    "    return str(hash(chunk.get(\"text\", \"\")))\n",
    "\n",
    "def _l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=-1, keepdims=True)\n",
    "    return x / (n + eps)\n",
    "\n",
    "def _pad_or_truncate_rows(mat: np.ndarray, k: int) -> np.ndarray:\n",
    "    mat = np.asarray(mat, dtype=np.float32)\n",
    "    if mat.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D matrix, got {mat.shape}\")\n",
    "    if mat.shape[0] == k:\n",
    "        return mat\n",
    "    if mat.shape[0] > k:\n",
    "        return mat[:k, :]\n",
    "    pad = np.zeros((k - mat.shape[0], mat.shape[1]), dtype=np.float32)\n",
    "    return np.vstack([mat, pad])\n",
    "\n",
    "def _load_chunks_and_embs(chunks_path: str, emb_path: str):\n",
    "    chunks = _load_json(chunks_path)\n",
    "    embs   = _load_json(emb_path)\n",
    "\n",
    "    # embeddings may be dict keyed by chunk_id OR list aligned with chunks\n",
    "    if isinstance(embs, dict):\n",
    "        aligned_chunks = []\n",
    "        emb_list = []\n",
    "        for ch in chunks:\n",
    "            cid = _get_chunk_id(ch)\n",
    "            if cid in embs:\n",
    "                aligned_chunks.append(ch)\n",
    "                emb_list.append(embs[cid])\n",
    "        chunks = aligned_chunks\n",
    "        P = np.array(emb_list, dtype=np.float32)\n",
    "    elif isinstance(embs, list):\n",
    "        P = np.array(embs, dtype=np.float32)\n",
    "        if P.shape[0] != len(chunks):\n",
    "            raise ValueError(\"Embeddings list length != chunks length.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown embeddings JSON format.\")\n",
    "\n",
    "    P = _l2_normalize(P.astype(np.float32))\n",
    "    return chunks, P\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build matrices from scratch:\n",
    "#   Contriever(q) -> retrieve topK in pre-embedded corpus -> cross-encoder rerank\n",
    "#   Extract last-layer [CLS] hidden vector for each (q, p_i)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def _mean_pool(last_hidden, mask):\n",
    "    mask = mask.unsqueeze(-1)\n",
    "    summed = (last_hidden * mask).sum(dim=1)\n",
    "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "    return summed / denom\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_query_contriever(query: str, retr_tok, retr_model) -> np.ndarray:\n",
    "    inp = retr_tok(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(DEVICE)\n",
    "    out = retr_model(**inp)\n",
    "    emb = _mean_pool(out.last_hidden_state, inp[\"attention_mask\"])\n",
    "    emb = F.normalize(emb, p=2, dim=-1)\n",
    "    return emb.squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "def dense_search(q_emb: np.ndarray, P: np.ndarray, top_k: int):\n",
    "    sims = P @ q_emb  # cosine if both L2-normalized\n",
    "    if top_k >= sims.shape[0]:\n",
    "        idxs = np.argsort(-sims)\n",
    "    else:\n",
    "        idxs = np.argpartition(-sims, top_k-1)[:top_k]\n",
    "        idxs = idxs[np.argsort(-sims[idxs])]\n",
    "    return idxs.tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_and_get_cls_and_ids(query: str, candidate_chunks: list, rr_tok, rr_model) -> Tuple[np.ndarray, List[str]]:\n",
    "    pairs = [[query, c.get(\"text\",\"\")] for c in candidate_chunks]\n",
    "    inp = rr_tok(pairs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(DEVICE)\n",
    "    out = rr_model(**inp, output_hidden_states=True)\n",
    "\n",
    "    scores = out.logits.squeeze(-1)         # (K,)\n",
    "    last_hidden = out.hidden_states[-1]     # (K,L,H)\n",
    "    cls = last_hidden[:, 0, :]              # (K,H)\n",
    "\n",
    "    order = torch.argsort(scores, descending=True)\n",
    "    cls = cls[order]\n",
    "\n",
    "    cand_ids = [_get_chunk_id(c) for c in candidate_chunks]\n",
    "    cand_ids = [cand_ids[i] for i in order.detach().cpu().tolist()]\n",
    "\n",
    "    return cls.detach().cpu().numpy().astype(np.float32), cand_ids\n",
    "\n",
    "def build_dataset_npz(force_rebuild: bool = False):\n",
    "    if DATASET_NPZ.exists() and not force_rebuild:\n",
    "        print(f\"[build] dataset exists: {DATASET_NPZ}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[build] Building matrices from scratch | device={DEVICE} | K={K}\")\n",
    "    retr_tok   = AutoTokenizer.from_pretrained(RETRIEVER_MODEL_NAME)\n",
    "    retr_model = AutoModel.from_pretrained(RETRIEVER_MODEL_NAME).to(DEVICE).eval()\n",
    "\n",
    "    rr_tok   = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)\n",
    "    rr_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME).to(DEVICE).eval()\n",
    "\n",
    "    D = int(getattr(rr_model.config, \"hidden_size\", -1))\n",
    "    if D <= 0:\n",
    "        raise ValueError(\"Could not read reranker hidden_size.\")\n",
    "    print(f\"[build] Reranker hidden_size D={D}\")\n",
    "\n",
    "    ben_chunks, ben_P = _load_chunks_and_embs(CHUNKS_JSON_PATH_BENIGN, EMB_JSON_PATH_BENIGN)\n",
    "    mal_chunks, mal_P = _load_chunks_and_embs(CHUNKS_JSON_PATH_MAL,    EMB_JSON_PATH_MAL)\n",
    "\n",
    "    queries = _parse_queries(_load_json(QUERY_LIST_PATH))\n",
    "    print(f\"[build] queries={len(queries)} (we build 2 runs per query => 2*Q runs)\")\n",
    "\n",
    "    X_list, y_list, qid_list, chunkids_list = [], [], [], []\n",
    "\n",
    "    def run_side(qid: int, query: str, chunks: list, P: np.ndarray, label: int):\n",
    "        q_emb = encode_query_contriever(query, retr_tok, retr_model)\n",
    "        idxs  = dense_search(q_emb, P, top_k=K)\n",
    "        cand  = [chunks[i] for i in idxs]\n",
    "\n",
    "        cls_mat, reranked_ids = rerank_and_get_cls_and_ids(query, cand, rr_tok, rr_model)  # (K,D)\n",
    "        cls_mat = _pad_or_truncate_rows(cls_mat, K)\n",
    "\n",
    "        X_list.append(cls_mat)\n",
    "        y_list.append(label)\n",
    "        qid_list.append(qid)\n",
    "        chunkids_list.append(reranked_ids)\n",
    "\n",
    "    for i, it in enumerate(queries, 1):\n",
    "        qid, qtext = it[\"qid\"], it[\"query\"]\n",
    "\n",
    "        # Run on benign corpus\n",
    "        run_side(qid, qtext, ben_chunks, ben_P, label=0)\n",
    "\n",
    "        # Run on malicious corpus\n",
    "        run_side(qid, qtext, mal_chunks, mal_P, label=1)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"[build] {i}/{len(queries)} queries done\")\n",
    "\n",
    "    X = np.stack(X_list, axis=0).astype(np.float32)      # (N,K,D)\n",
    "    y = np.array(y_list, dtype=np.int64)                 # (N,)\n",
    "    qids = np.array(qid_list, dtype=np.int64)            # (N,)\n",
    "    chunk_ids = np.array(chunkids_list, dtype=object)    # (N,) each is list length K\n",
    "\n",
    "    # Balance classes for training stability\n",
    "    idx0 = np.where(y==0)[0]\n",
    "    idx1 = np.where(y==1)[0]\n",
    "    n = min(len(idx0), len(idx1))\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    idx0 = rng.choice(idx0, size=n, replace=False)\n",
    "    idx1 = rng.choice(idx1, size=n, replace=False)\n",
    "    idx = np.concatenate([idx0, idx1])\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    X = X[idx]; y = y[idx]; qids = qids[idx]; chunk_ids = chunk_ids[idx]\n",
    "\n",
    "    np.savez_compressed(DATASET_NPZ, X=X, y=y, qids=qids, chunk_ids=chunk_ids)\n",
    "    print(f\"[build] saved {DATASET_NPZ}\")\n",
    "    print(f\"        X={X.shape} y={y.shape} unique_qids={len(np.unique(qids))}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Split by qid (prevents leakage of same query across splits)\n",
    "# ============================================================\n",
    "def split_by_qid(qids: np.ndarray, train=0.7, val=0.15, seed=SEED):\n",
    "    uq = np.unique(qids)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(uq)\n",
    "    n = len(uq)\n",
    "    n_tr = int(train*n)\n",
    "    n_va = int(val*n)\n",
    "\n",
    "    tr_q = uq[:n_tr]\n",
    "    va_q = uq[n_tr:n_tr+n_va]\n",
    "    te_q = uq[n_tr+n_va:]\n",
    "\n",
    "    tr_mask = np.isin(qids, tr_q)\n",
    "    va_mask = np.isin(qids, va_q)\n",
    "    te_mask = np.isin(qids, te_q)\n",
    "    return tr_mask, va_mask, te_mask\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SupCon loss (run labels only)\n",
    "# ============================================================\n",
    "def supervised_contrastive_loss(z: torch.Tensor, y: torch.Tensor, tau: float = 0.2) -> torch.Tensor:\n",
    "    B = z.size(0)\n",
    "    sim = (z @ z.t()) / max(tau, 1e-6)\n",
    "    sim = sim - torch.eye(B, device=z.device) * 1e9\n",
    "\n",
    "    y = y.view(-1, 1)\n",
    "    mask_pos = (y == y.t()).float()\n",
    "    mask_pos = mask_pos - torch.eye(B, device=z.device)\n",
    "\n",
    "    exp_sim = torch.exp(sim)\n",
    "    denom = exp_sim.sum(dim=1, keepdim=True).clamp(min=1e-9)\n",
    "    log_prob = sim - torch.log(denom)\n",
    "\n",
    "    pos_count = mask_pos.sum(dim=1).clamp(min=1.0)\n",
    "    loss = -(mask_pos * log_prob).sum(dim=1) / pos_count\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model: SetTransformer encoder + per-chunk head + attention MIL pooling\n",
    "# ============================================================\n",
    "class SetTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1  = nn.LayerNorm(d_model)\n",
    "        self.ff   = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "        )\n",
    "        self.ln2  = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.attn(x, x, x, need_weights=False)\n",
    "        x = self.ln1(x + self.drop(h))\n",
    "        h = self.ff(x)\n",
    "        x = self.ln2(x + self.drop(h))\n",
    "        return x\n",
    "\n",
    "class AttentionMILPooling(nn.Module):\n",
    "    def __init__(self, d_model: int, d_attn: int = 128):\n",
    "        super().__init__()\n",
    "        self.V = nn.Linear(d_model, d_attn)\n",
    "        self.w = nn.Linear(d_attn, 1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        A = self.w(torch.tanh(self.V(H))).squeeze(-1)  # (B,K)\n",
    "        a = torch.softmax(A, dim=1)                    # (B,K)\n",
    "        bag = (a.unsqueeze(-1) * H).sum(dim=1)         # (B,d)\n",
    "        return a, bag\n",
    "\n",
    "class MIL_SetTransformer_AttnMIL(nn.Module):\n",
    "    def __init__(self, D_in: int, d_model: int, n_heads: int, n_layers: int,\n",
    "                 dropout: float, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(D_in, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SetTransformerBlock(d_model, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.chunk_head = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.pool = AttentionMILPooling(d_model, d_attn=128)\n",
    "        self.run_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "        # For SupCon embedding (run-level)\n",
    "        self.emb_head = nn.Sequential(\n",
    "            nn.Linear(2*d_model, 2*d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*d_model, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, M):\n",
    "        # M: (B,K,D)\n",
    "        H = self.proj(M)\n",
    "        for blk in self.blocks:\n",
    "            H = blk(H)\n",
    "\n",
    "        chunk_logits = self.chunk_head(H).squeeze(-1)  # (B,K)\n",
    "        s = torch.sigmoid(chunk_logits)                # (B,K)\n",
    "\n",
    "        a, bag = self.pool(H)\n",
    "        run_logit = self.run_head(bag).squeeze(-1)     # (B,)\n",
    "        y_hat = torch.sigmoid(run_logit)\n",
    "\n",
    "        h_mean = H.mean(dim=1)\n",
    "        h_max  = H.max(dim=1).values\n",
    "        g = torch.cat([h_mean, h_max], dim=1)          # (B,2d)\n",
    "        emb = self.emb_head(g)\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "        return chunk_logits, s, a, y_hat, emb\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class RunDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Metrics + eval-only chunk GT\n",
    "# ============================================================\n",
    "def _counts_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def _rates(tp, tn, fp, fn):\n",
    "    eps = 1e-9\n",
    "    acc  = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    tpr  = tp / (tp + fn + eps)\n",
    "    fpr  = fp / (fp + tn + eps)\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    return {\"acc\": acc, \"tpr\": tpr, \"fpr\": fpr, \"prec\": prec}\n",
    "\n",
    "def _chunk_gt_eval_only(y_run: np.ndarray, chunk_ids_obj: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Eval-only chunk GT:\n",
    "      - benign runs: all 0\n",
    "      - malicious runs: chunk=1 if \"gpt\" in chunk_id (case-insensitive)\n",
    "    \"\"\"\n",
    "    N = len(y_run)\n",
    "    gt = np.zeros((N, K), dtype=np.int64)\n",
    "    for i in range(N):\n",
    "        if int(y_run[i]) == 0:\n",
    "            continue\n",
    "        ids = chunk_ids_obj[i]\n",
    "        for j, cid in enumerate(ids):\n",
    "            if cid is None:\n",
    "                continue\n",
    "            if \"gpt\" in str(cid).lower():\n",
    "                gt[i, j] = 1\n",
    "    return gt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Calibration for thr_loc (benign val only, no chunk labels)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def calibrate_thr_loc_benign_only(model: nn.Module, X_val: np.ndarray, y_val: np.ndarray,\n",
    "                                  target_chunk_fpr: float) -> float:\n",
    "    \"\"\"\n",
    "    Choose thr_loc so that on benign VAL runs (y=0), about target_chunk_fpr\n",
    "    of chunk scores exceed thr_loc. (No chunk labels needed.)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X = torch.from_numpy(X_val.astype(np.float32)).to(DEVICE)\n",
    "    y = y_val.astype(np.int64)\n",
    "\n",
    "    _, s, _, _, _ = model(X)\n",
    "    s_np = s.detach().cpu().numpy()  # (N,K)\n",
    "\n",
    "    neg_scores = s_np[y == 0].reshape(-1)\n",
    "    if neg_scores.size == 0:\n",
    "        return 0.5\n",
    "\n",
    "    q = max(0.0, min(1.0, 1.0 - float(target_chunk_fpr)))\n",
    "    thr = float(np.quantile(neg_scores, q))\n",
    "    return thr\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GATED-ONLY evaluation (what you asked for)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_gated_only(model: nn.Module, X_np: np.ndarray, y_np: np.ndarray, chunk_ids_obj: np.ndarray,\n",
    "                        thr_det: float, thr_loc: float,\n",
    "                        use_topr: bool = False, top_r: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    - Run-level detection metrics\n",
    "    - Chunk-level eval-only metrics with GATING:\n",
    "        if run_pred==0 => predict all-0 chunks\n",
    "        if run_pred==1 => predict chunks via threshold OR top-r\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X = torch.from_numpy(X_np.astype(np.float32)).to(DEVICE)\n",
    "    y = y_np.astype(np.int64)\n",
    "\n",
    "    chunk_logits, s, _, y_hat, _ = model(X)\n",
    "\n",
    "    run_prob = y_hat.detach().cpu().numpy()\n",
    "    run_pred = (run_prob >= thr_det).astype(np.int64)\n",
    "\n",
    "    # Run-level metrics\n",
    "    tp, tn, fp, fn = _counts_from_preds(y, run_pred)\n",
    "    run_report = {\n",
    "        \"counts\": {\"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn},\n",
    "        \"rates\": _rates(tp, tn, fp, fn),\n",
    "        \"thr_det\": float(thr_det),\n",
    "        \"pooling\": \"AttentionMIL(Ilse2018)\",\n",
    "    }\n",
    "\n",
    "    # Chunk preds\n",
    "    s_np = s.detach().cpu().numpy()  # (N,K)\n",
    "\n",
    "    pred_chunk = np.zeros_like(s_np, dtype=np.int64)\n",
    "\n",
    "    if use_topr:\n",
    "        # For runs predicted poisoned, mark top-r by score\n",
    "        for i in range(len(s_np)):\n",
    "            if run_pred[i] == 0:\n",
    "                continue\n",
    "            idx = np.argsort(-s_np[i])[:top_r]\n",
    "            pred_chunk[i, idx] = 1\n",
    "        loc_mode = f\"topr(r={top_r})\"\n",
    "    else:\n",
    "        pred_chunk = (s_np >= thr_loc).astype(np.int64)\n",
    "        # GATING\n",
    "        pred_chunk[run_pred == 0, :] = 0\n",
    "        loc_mode = f\"threshold(thr_loc={thr_loc:.6f})\"\n",
    "\n",
    "    # Eval-only GT\n",
    "    gt_chunk = _chunk_gt_eval_only(y, chunk_ids_obj)\n",
    "\n",
    "    gt_f = gt_chunk.reshape(-1)\n",
    "    pr_f = pred_chunk.reshape(-1)\n",
    "\n",
    "    tp2, tn2, fp2, fn2 = _counts_from_preds(gt_f, pr_f)\n",
    "    chunk_report = {\n",
    "        \"counts\": {\"tp\": tp2, \"tn\": tn2, \"fp\": fp2, \"fn\": fn2},\n",
    "        \"rates\": _rates(tp2, tn2, fp2, fn2),\n",
    "        \"localization\": loc_mode,\n",
    "        \"gating\": \"Only localize when run_pred==1\",\n",
    "        \"gt_rule\": \"eval-only: ('gpt' in chunk_id) for malicious runs; all-0 for benign runs\",\n",
    "    }\n",
    "\n",
    "    return {\"run_level\": run_report, \"chunk_level_eval_only_gated\": chunk_report}\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Train (run-label-only) + test report (gated-only)\n",
    "# ============================================================\n",
    "def train_and_test():\n",
    "    print(f\"[out] OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "    if BUILD_MATRICES:\n",
    "        build_dataset_npz(force_rebuild=FORCE_REBUILD)\n",
    "\n",
    "    data = np.load(DATASET_NPZ, allow_pickle=True)\n",
    "    X = data[\"X\"].astype(np.float32)         # (N,K,D)\n",
    "    y = data[\"y\"].astype(np.int64)           # (N,)\n",
    "    qids = data[\"qids\"].astype(np.int64)     # (N,)\n",
    "    chunk_ids = data[\"chunk_ids\"]            # (N,) object\n",
    "\n",
    "    N, K_, D = X.shape\n",
    "    assert K_ == K\n",
    "    print(f\"[data] X={X.shape} y={y.shape} unique_qids={len(np.unique(qids))}\")\n",
    "\n",
    "    tr_mask, va_mask, te_mask = split_by_qid(qids, train=TRAIN_SPLIT, val=VAL_SPLIT, seed=SEED)\n",
    "    X_tr, y_tr = X[tr_mask], y[tr_mask]\n",
    "    X_va, y_va, ids_va = X[va_mask], y[va_mask], chunk_ids[va_mask]\n",
    "    X_te, y_te, ids_te = X[te_mask], y[te_mask], chunk_ids[te_mask]\n",
    "    print(f\"[split] train={len(X_tr)} val={len(X_va)} test={len(X_te)}\")\n",
    "\n",
    "    train_loader = DataLoader(RunDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    model = MIL_SetTransformer_AttnMIL(\n",
    "        D_in=D, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,\n",
    "        dropout=DROPOUT, embed_dim=EMBED_DIM\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    bce = nn.BCELoss()\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    best_opt_state = None\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tot = 0.0\n",
    "        nseen = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            yb_f = yb.float()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            chunk_logits, s, a, y_hat, emb = model(xb)\n",
    "\n",
    "            # RUN-label-only losses\n",
    "            L_det = bce(y_hat, yb_f)\n",
    "\n",
    "            mean_s = s.mean(dim=1)                # (B,)\n",
    "            max_s  = s.max(dim=1).values          # (B,)\n",
    "            lse = torch.logsumexp(chunk_logits, dim=1) - math.log(K)  # (B,)\n",
    "\n",
    "            neg_mask = (1 - yb_f)  # y=0\n",
    "            pos_mask = yb_f        # y=1\n",
    "\n",
    "            # Benign suppression (lower chunk FPR)\n",
    "            L_neg_mean = (mean_s * neg_mask).mean()\n",
    "            L_neg_max  = (max_s  * neg_mask).mean()\n",
    "            L_neg_lse  = (torch.sigmoid(lse) * neg_mask).mean()\n",
    "\n",
    "            # Positive coverage (keep chunk TPR)\n",
    "            L_cov = (F.relu(RHO_COV - mean_s) * pos_mask).mean()\n",
    "\n",
    "            # Attention entropy on positives (avoid collapse when many chunks are poison)\n",
    "            eps = 1e-9\n",
    "            ent = -(a * (a + eps).log()).sum(dim=1)  # (B,)\n",
    "            L_attn_ent = ((-ent) * pos_mask).mean()  # maximize entropy on positives\n",
    "\n",
    "            # SupCon (optional)\n",
    "            if USE_SUPCON:\n",
    "                L_sc = supervised_contrastive_loss(emb, yb, tau=SUPCON_TAU)\n",
    "            else:\n",
    "                L_sc = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = (\n",
    "                W_DET * L_det\n",
    "                + W_COV * L_cov\n",
    "                + W_NEG_MEAN * L_neg_mean\n",
    "                + W_NEG_MAX  * L_neg_max\n",
    "                + W_NEG_LSE  * L_neg_lse\n",
    "                + W_ATTN_ENT * L_attn_ent\n",
    "                + W_SUPCON * L_sc\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            opt.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            tot += float(loss.item()) * bs\n",
    "            nseen += bs\n",
    "\n",
    "        train_loss = tot / max(nseen, 1)\n",
    "\n",
    "        # Val model selection uses ONLY run BCE (still no chunk labels)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Xv = torch.from_numpy(X_va.astype(np.float32)).to(DEVICE)\n",
    "            yv = torch.from_numpy(y_va.astype(np.float32)).to(DEVICE)\n",
    "            _, _, _, yhat_v, _ = model(Xv)\n",
    "            val_loss = float(bce(yhat_v, yv).item())\n",
    "\n",
    "        print(f\"[ep {ep:02d}] train_loss={train_loss:.6f} val_runBCE={val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            best_opt_state = opt.state_dict()\n",
    "\n",
    "            torch.save({\n",
    "                \"model_state_dict\": best_state,\n",
    "                \"optimizer_state_dict\": best_opt_state,\n",
    "                \"K\": int(K),\n",
    "                \"D_EXPECTED\": int(D),\n",
    "                \"embed_dim\": int(EMBED_DIM),\n",
    "                \"best_val_loss\": float(best_val),\n",
    "                \"seed\": int(SEED),\n",
    "                \"reranker_model_name\": str(RERANKER_MODEL_NAME),\n",
    "                \"reranker_tag\": str(RERANKER_TAG),\n",
    "                \"note\": \"GATEDONLY: SetTransformer+AttnMIL. Train/Val use RUN labels only. Chunk GT used only for TEST eval.\",\n",
    "            }, BEST_CKPT_PTH)\n",
    "            print(f\"  -> saved BEST: {BEST_CKPT_PTH}\")\n",
    "\n",
    "    # Load best model\n",
    "    ckpt = torch.load(BEST_CKPT_PTH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Calibrate thr_loc (benign val only, no chunk labels)\n",
    "    if not USE_TOPR_INSTEAD_OF_THRESHOLD:\n",
    "        thr_loc_raw = calibrate_thr_loc_benign_only(\n",
    "            model, X_va, y_va, target_chunk_fpr=TARGET_CHUNK_FPR_BENIGN_VAL\n",
    "        )\n",
    "        thr_loc = float(np.clip(thr_loc_raw, MIN_THR_LOC, MAX_THR_LOC))\n",
    "    else:\n",
    "        thr_loc_raw = None\n",
    "        thr_loc = None\n",
    "\n",
    "    calib_obj = {\n",
    "        \"thr_det\": float(THR_DET),\n",
    "        \"use_topr_instead_of_threshold\": bool(USE_TOPR_INSTEAD_OF_THRESHOLD),\n",
    "        \"top_r\": int(TOP_R),\n",
    "        \"target_chunk_fpr_benign_val\": float(TARGET_CHUNK_FPR_BENIGN_VAL),\n",
    "        \"thr_loc_raw\": None if thr_loc_raw is None else float(thr_loc_raw),\n",
    "        \"thr_loc_clamped\": None if thr_loc is None else float(thr_loc),\n",
    "        \"thr_loc_clamp_min\": float(MIN_THR_LOC),\n",
    "        \"thr_loc_clamp_max\": float(MAX_THR_LOC),\n",
    "        \"reranker_model_name\": str(RERANKER_MODEL_NAME),\n",
    "        \"reranker_tag\": str(RERANKER_TAG),\n",
    "    }\n",
    "    with open(CALIB_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(calib_obj, f, indent=2)\n",
    "    print(f\"[calib] saved {CALIB_JSON}\")\n",
    "\n",
    "    # Test report (GATED ONLY)\n",
    "    if USE_TOPR_INSTEAD_OF_THRESHOLD:\n",
    "        report = evaluate_gated_only(\n",
    "            model, X_te, y_te, ids_te,\n",
    "            thr_det=THR_DET, thr_loc=0.5,\n",
    "            use_topr=True, top_r=TOP_R\n",
    "        )\n",
    "    else:\n",
    "        report = evaluate_gated_only(\n",
    "            model, X_te, y_te, ids_te,\n",
    "            thr_det=THR_DET, thr_loc=thr_loc,\n",
    "            use_topr=False, top_r=TOP_R\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== TEST REPORT (run + chunk eval-only, GATED ONLY) ===\")\n",
    "    print(report)\n",
    "\n",
    "    with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"[report] saved {REPORT_JSON}\")\n",
    "\n",
    "    return model, report\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37982f-e3b9-466c-8080-b07573fc8d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
